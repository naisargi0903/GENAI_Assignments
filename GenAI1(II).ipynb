{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtRQDQ_KR1JF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# Builds an n-gram Markov Chain model from the input text.\n",
        "def build_markov_chain(text, n=2):\n",
        "    words = text.split()\n",
        "    markov_chain = {}\n",
        "\n",
        "    for i in range(len(words) - n):\n",
        "        key = tuple(words[i:i + n])  # n-gram key\n",
        "        next_word = words[i + n]\n",
        "\n",
        "        if key not in markov_chain:\n",
        "            markov_chain[key] = []\n",
        "        markov_chain[key].append(next_word)\n",
        "\n",
        "    return markov_chain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates text using the Markov Chain model.\n",
        "def generate_text(markov_chain, num_words=50, n=2):\n",
        "    key = random.choice(list(markov_chain.keys()))  # Start with a random n-gram\n",
        "    output = list(key)\n",
        "\n",
        "    for _ in range(num_words - n):\n",
        "        if key in markov_chain:\n",
        "            next_word = random.choice(markov_chain[key])\n",
        "            output.append(next_word)\n",
        "            key = tuple(output[-n:])  # Move to next n-gram\n",
        "        else:\n",
        "            break  # Stop if no next word\n",
        "\n",
        "    return \" \".join(output)"
      ],
      "metadata": {
        "id": "tt_IlTLvT9QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take user input for training text\n",
        "user_text = input(\"Enter the training text: \")\n",
        "\n",
        "# Choose n-gram size\n",
        "n = int(input(\"Enter n-gram size (default is 2): \") or 2)\n",
        "\n",
        "# Choose number of words in output\n",
        "num_words = int(input(\"Enter number of words to generate (default is 30): \") or 30)\n",
        "\n",
        "# Build and generate text\n",
        "markov_chain = build_markov_chain(user_text, n)\n",
        "generated_text = generate_text(markov_chain, num_words, n)\n",
        "\n",
        "# Display generated text\n",
        "print(\"\\nGenerated Text:\\n\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylu0FZQeUDeu",
        "outputId": "fe3b4c8c-e6ce-49d2-c705-2073985bfd5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the training text: It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using 'Content here, content here', making it look like readable English. Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for 'lorem ipsum' will uncover many web sites still in their infancy.\n",
            "Enter n-gram size (default is 2): 3\n",
            "Enter number of words to generate (default is 30): 50\n",
            "\n",
            "Generated Text:\n",
            " a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using 'Content here, content here', making it look like\n"
          ]
        }
      ]
    }
  ]
}